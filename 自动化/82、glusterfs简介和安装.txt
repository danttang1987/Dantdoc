82、glusterfs简介和安装

1、配置主机名及host或DNS解析

2、安装epel-release

yum install epel-release -y


yum install -y flex bison openssl openssl-devel acl libacl libacl-devel sqlite-devel libxml2-devel python-devel make cmake gcc gcc-c++ autoconf automake libtool unzip zip

http://buildlogs.centos.org/centos/7/storage/x86_64/gluster-4.0/

配置指版本的yum

可配置这个路径为yum源
https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-4.0/

也可使用yum安装指版本的rpm包
centos-release-gluster40.x86_64
centos-release-gluster41.noarch
centos-release-gluster5.noarch 
centos-release-gluster6.noarch


yum install glusterfs* -y


yum install glusterfs-server glusterfs-cli glusterfs-geo-replication


glusterfs配置
glusterfsf -v

systemctl start glusterd

gluster peer probe storage02
gluster peer probe storage03
gluster peer probe storage04

gluster peer status

mkfs.xfs /dev/sdc

mkdir /storage/{black1,black2} -p

echo "/dev/sdb    /storage/brick1    xfs    defaults    0 0" >> /etc/fstab
echo "/dev/sdc    /storage/brick2    xfs    defaults    0 0" >> /etc/fstab


mount -a


Distributed:分布式卷
replicated:复制卷
striped:条带
Distributed striped
Distributed replicated

gluster volume create gv1 storage01:/storage/brick1 storage02:/storage/brick1 force

gluster volume start gv1

gluster volume status   
	Status of volume: gv1
	Gluster process                             TCP Port  RDMA Port  Online  Pid
	------------------------------------------------------------------------------
	Brick storage01:/storage/brick1             49152     0          Y       38737
	Brick storage02:/storage/brick1             49152     0          Y       38665
	
	Task Status of Volume gv1
	------------------------------------------------------------------------------
	There are no active volume tasks


gluster volume info
	Volume Name: gv1
	Type: Distribute
	Volume ID: 58397bf0-2aa9-4a61-8aa2-8dc92c14a59e
	Status: Started
	Snapshot Count: 0
	Number of Bricks: 2
	Transport-type: tcp
	Bricks:
	Brick1: storage01:/storage/brick1
	Brick2: storage02:/storage/brick1
	Options Reconfigured:
	transport.address-family: inet
	nfs.disable: on


mount -t glusterfs 192.168.237.212:/gv1 /data/gv1


mount -o mountproto=tcp -t nfs 192.168.237.212:/gv1 /data/gv1


gluster volume create gv2 replica 2 storage03:/storage/brick1 storage04:/storage/brick1 force

gluster volume start gv2


gluster volume create gv3 stripe 2 storage03:/storage/brick2 storage04:/storage/brick2 force

gluster volume start gv3


添加磁盘
gluster volume add-brick gv2 replica 2 storage01:/storage/brick2 storage02:/storage/brick2 force

重新平衡数据
gluster volume rebalance gv2 start

gluster volume rebalance gv2 status


移出brick并迁移数据
gluster volume remove-brick gv2 replica 2 storage01:/storage/brick1 storage02:/storage/brick1 start
gluster volume remove-brick gv2 replica 2 storage01:/storage/brick1 storage02:/storage/brick1 status
gluster volume remove-brick gv2 replica 2 storage01:/storage/brick1 storage02:/storage/brick1 commit


优化
gluster volume quota gv2 enable
gluster volume quota gv2 limit-usage /d1 5GB
gluster volume quota gv2 list
gluster volume quota gv2 remove /d1


gluster volume set gv2 performance.cache-size 128MB

gluster volume set gv2 performance.flush-behind on

gluster volume set gv2 performance.io-thread-count 32

gluster volume set gv2 performance.write-behind on


将节点移出GlusterFS集群，可以批量移除。如下将glusterfs3和glusterfs4两个节点移除集群。
如果是副本卷，移除的节点需要时replica的整数倍。
默认情况下节点是移除不了的,可以使用force强制移除（不建议强制移除节点）。
前提是移除的节点上的brick要移除。
gluster peer detach glusterfs3 glusterfs4  force



gluster volume set gfs auth.allow 192.168.*



#将glusterfs3的数据迁移到glusterfs5,先将glusterfs5加入集群
[root@GlusterFS-master ~]# gluster peer probe glusterfs5  
 
#开始迁移
[root@GlusterFS-master ~]# gluster volume replace-brick gv0 glusterfs3:/data/brick1/gv0 glusterfs5:/data/brick1/gv0 start 
 
#查看迁移状态 
[root@GlusterFS-master ~]# gluster volume replace-brick gv0 glusterfs3:/data/brick1/gv0 glusterfs5:/data/brick1/gv0 status
 
#数据迁移完毕后提交  
[root@GlusterFS-master ~]# gluster volume replace-brick gv0 glusterfs3:/data/brick1/gv0 glusterfs5:/data/brick1/gv0 commit   
 
#如果机器glusterfs3出现故障已经不能运行,执行强制提交  
[root@GlusterFS-master ~]# gluster volume replace-brick gv0 glusterfs3:/data/brick1/gv0 glusterfs5:/data/brick1/gv0 commit force
 
#同步整个卷  
[root@GlusterFS-master ~]# gluster volume heal gfs full



硬盘故障处理

[root@storage01 brick2]# getfattr -d -m '.*' /storage/brick2
getfattr: Removing leading '/' from absolute path names
# file: storage/brick2
trusted.afr.dirty=0sAAAAAAAAAAAAAAAA
trusted.afr.gv2-client-3=0sAAAAAAAAAAAAAAAB
trusted.gfid=0sAAAAAAAAAAAAAAAAAAAAAQ==
trusted.glusterfs.dht=0s5e+q+QAAAAAAAAAAVVVVVA==
trusted.glusterfs.dht.commithash="3857689337"
trusted.glusterfs.quota.dirty=0sMAA=
trusted.glusterfs.quota.size.1=0sAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAC
trusted.glusterfs.volume-id=0sJfQPKqf0RMm7QNQTm5wCZw==

[root@storage02 storage]# getfattr -d -m '.*' /storage/brick2
[root@storage02 storage]# 



setfattr -n trusted.glusterfs.volume-id -v 0sJfQPKqf0RMm7QNQTm5wCZw== /storage/brick2
setfattr -n trusted.gfid -v 0sAAAAAAAAAAAAAAAAAAAAAQ== /storage/brick2
setfattr -n trusted.afr.dirty -v 0sAAAAAAAAAAAAAAAA /storage/brick2
setfattr -n trusted.glusterfs.dht -v 0s5e+q+QAAAAAAAAAAVVVVVA== /storage/brick2
setfattr -n trusted.glusterfs.dht.commithash -v "3857689337" /storage/brick2

setfattr -n trusted.afr.gv2-client-3 -v 0sAAAAAAAAAAAAAAAB /storage/brick2

setfattr -n trusted.glusterfs.quota.dirty -v 0sMAA= /storage/brick2
setfattr -n trusted.glusterfs.quota.size.1 -v 0sAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAC /storage/brick2



6）Brick管理
这里以一个例子来说明：把192.168.10.151:/mnt/brick0 替换为192.168.10.151:/mnt/brick2

6.1）开始替换
[root@GlusterFS-slave ~]# gluster volume replace-brick test-volume 192.168.10.151:/mnt/brick0 192.168.10.152:/mnt/brick2 start
异常信息：volume replace-brick: failed: /data/share2 or a prefix of it is already part of a volume 

说明 /mnt/brick2 曾经是一个Brick。具体解决方法
[root@GlusterFS-slave ~]# rm -rf /mnt/brick2/.glusterfs

[root@GlusterFS-slave ~]# setfattr -x trusted.glusterfs.volume-id /mnt/brick2
[root@GlusterFS-slave ~]# setfattr -x trusted.gfid  /mnt/brick2

//如上，执行replcace-brick卷替换启动命令，使用start启动命令后，开始将原始Brick的数据迁移到即将需要替换的Brick上。

6.2）查看是否替换完
[root@GlusterFS-slave ~]# gluster volume replace-brick test-volume 192.168.10.151:/mnt/brick0 192.168.10.152:/mnt/brick2 status

6.3）在数据迁移的过程中，可以执行abort命令终止Brick替换。
[root@GlusterFS-slave ~]# gluster volume replace-brick test-volume 192.168.10.151:/mnt/brick0 192.168.10.152:/mnt/brick2 abort

6.4）在数据迁移结束之后，执行commit命令结束任务，则进行Brick替换。使用volume info命令可以查看到Brick已经被替换。
[root@GlusterFS-slave ~]# gluster volume replace-brick test-volume 192.168.10.151:/mnt/brick0 192.168.10.152:/mnt/brick2 commit
此时再往 /sf/data/vs/gfs/rep2上添加数据的话，数据会同步到 192.168.10.152:/mnt/brick0和192.168.10.152:/mnt/brick2上。而不会同步到
192.168.10.151:/mnt/brick0 上。




